{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965840e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from mne.stats import fdr_correction\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random \n",
    "import subprocess\n",
    "import matplotlib as mpl\n",
    "from matplotlib import animation\n",
    "from sqlalchemy import create_engine, types, dialects\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style('ticks')\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "plt.rcParams['svg.fonttype'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ca73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples metadata\n",
    "filtered_reads_samples_filterdis  = pd.read_pickle('.../metadata.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e681af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use human/bacterial protein abundance profile to run RF\n",
    "all_uniref_human_profile = glob('hp_abun/*human_cluster_filter.pkl')\n",
    "all_uniref_human_profile_df = [pd.read_pickle(x) for x in all_uniref_human_profile]\n",
    "\n",
    "# all study names\n",
    "study_name = pd.DataFrame(all_uniref_human_profile)[0].str.replace('hp_abun/','').str.replace('_human_cluster_filter.pkl','').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach \"labels\" to the metagenomic samples\n",
    "def return_all_factorized():\n",
    "    all_df_study = []\n",
    "    for i in range(len(study_name)):\n",
    "        temp = all_uniref_human_profile_df[i].reset_index()\\\n",
    "        .merge(filtered_reads_samples_filterdis[filtered_reads_samples_filterdis.study_name\\\n",
    "        == study_name[i]][['sample_id','study_condition']].dropna(), right_on = 'sample_id', \\\n",
    "        left_on = 'index').drop('index', axis = 1).rename(columns = {'study_condition':'y'})\n",
    "        temp.index = temp.sample_id\n",
    "        temp.drop('sample_id', axis = 1, inplace = True)\n",
    "        temp.loc[temp['y']!='control', 'y'] = 1\n",
    "        temp.loc[temp['y']=='control', 'y'] = 0\n",
    "        temp['y'] = temp['y'].astype('int')\n",
    "        temp = temp.reset_index(drop = True)\n",
    "        all_df_study.append(temp)\n",
    "    return all_df_study\n",
    "\n",
    "all_uniref_human_profile_df_fac = return_all_factorized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec64b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tuned hyperparams from gridsearch\n",
    "best_para = pd.read_pickle('RF_final/best_hyperparam.pkl')\n",
    "best_para = best_para.replace(np.nan, 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1760fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, average_precision_score, precision_score, recall_score, f1_score\n",
    "from scipy import interp\n",
    "\n",
    "def run_rf_eval(data, study_name_one):\n",
    "\n",
    "    X = data.iloc[:,:-1]\n",
    "    y = data['y']\n",
    "    results_all = []\n",
    "    \n",
    "    min_samples_leaf = best_para[best_para.study_name == study_name_one].min_samples_leaf.values[0]\n",
    "    min_samples_split = best_para[best_para.study_name == study_name_one].min_samples_split.values[0]\n",
    "    n_estimators = best_para[best_para.study_name == study_name_one].n_estimators.values[0]\n",
    "    max_depth = best_para[best_para.study_name == study_name_one].max_depth.values[0]\n",
    "    \n",
    "    for i in range(20):\n",
    "        if max_depth == 'None':\n",
    "            clf = RandomForestClassifier(n_jobs = 10, bootstrap = True, max_features = 'auto',\n",
    "                                     min_samples_leaf = min_samples_leaf,\n",
    "                                    min_samples_split = min_samples_split, \n",
    "                                    n_estimators = n_estimators, class_weight = 'balanced')\n",
    "        else:\n",
    "            clf = RandomForestClassifier(n_jobs = 10, bootstrap = True, max_features = 'auto',\n",
    "                                     min_samples_leaf = min_samples_leaf,\n",
    "                                    min_samples_split = min_samples_split, \n",
    "                                    n_estimators = n_estimators, max_depth = max_depth, class_weight = 'balanced')\n",
    "\n",
    "        results_all.append(pd.DataFrame(cross_validate(clf, X, y, cv=5, n_jobs = 30, scoring=('accuracy', 'average_precision', \n",
    "                                                                                              'roc_auc','precision','recall','f1'))))\n",
    "        results_all_df = pd.concat(results_all)\n",
    "        results_all_df['study'] = study_name_one\n",
    "    \n",
    "    print(study_name_one, \":done\")\n",
    "    \n",
    "    return results_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf25ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run 5fold CV\n",
    "all_results_eval = []\n",
    "for i in range(len(study_name)):\n",
    "    all_results_eval.append(run_rf_eval(all_uniref_human_profile_df_fac[i], study_name[i]))\n",
    "    \n",
    "pd.concat(all_results_eval).to_pickle('RF_final/5fold_CV.after_gridsearch.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239732a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std\n",
    "mean_table = pd.concat(all_results_eval).groupby(['study']).mean().drop(['fit_time','score_time', 'test_accuracy','test_average_precision'], axis =1)\n",
    "std_table = pd.concat(all_results_eval).groupby(['study']).std().drop(['fit_time','score_time', 'test_accuracy','test_average_precision'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0257b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the eval\n",
    "df = mean_table.sort_values('test_roc_auc', ascending = False)\n",
    "df.columns = ['AUROC','Precision','Recall','F1']\n",
    "sns.set(font_scale=1.1)\n",
    "sns.set_style('white')\n",
    "fig, ax = plt.subplots(figsize = (3,5))\n",
    "sns.heatmap(df, cmap='Oranges', annot=True, fmt='.2f', vmin = 0.4)\n",
    "plt.xticks(rotation=30, fontsize = 12)\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06bd8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbsubrito3",
   "language": "python",
   "name": "cbsubrito3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
